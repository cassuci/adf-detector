{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0676d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module contains useful functions and classes for computing quality evaluation metrics.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as skmetrics\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "class BinaryPerformance:\n",
    "    \"\"\"\n",
    "    Evaluate binary classification model performance and visualize results.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, y_true, y_score=None, threshold=0.5\n",
    "    ):  # pylint: disable=too-many-arguments\n",
    "        \"\"\"\n",
    "        Initialize the BinaryPerformance instance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_score : np.ndarray\n",
    "            Predicted scores from the model.\n",
    "        y_true : np.ndarray\n",
    "            True labels.\n",
    "        threshold : float, optional\n",
    "            Classification threshold. Defaults to 0.5.\n",
    "        \"\"\"\n",
    "        self.given_y_score = y_score\n",
    "        self.y_true = y_true\n",
    "        self.threshold = threshold\n",
    "        self.predictions = self.get_binary_predictions()\n",
    "\n",
    "    def get_binary_predictions(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate binary predictions and scores for a given model on a given dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            A DataFrame containing binary predictions and scores.\n",
    "        \"\"\"\n",
    "        if self.given_y_score is not None:\n",
    "            y_score = self.given_y_score\n",
    "            y_pred = (self.given_y_score >= self.threshold).astype(int)\n",
    "        else:\n",
    "            raise(ValueError(\"You need to provide y_score so I can generate binary predictions.\"))\n",
    "\n",
    "        predictions = pd.DataFrame({\"y_pred\": y_pred, \"y_score\": y_score})\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def evaluate_binary_performance(\n",
    "        self,\n",
    "        label: str = \"\",\n",
    "        add_prefix: bool = True,\n",
    "        verbose: bool = True,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Evaluate binary classification performance and return metrics.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        label : str, optional\n",
    "            A label for the performance report.\n",
    "        add_prefix : bool, optional\n",
    "            Whether to add the label as a prefix to metric keys.\n",
    "        verbose : bool, optional\n",
    "            Whether to print the performance report.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            A DataFrame containing evaluation metrics.\n",
    "        \"\"\"\n",
    "        ap_score = skmetrics.average_precision_score(self.y_true, self.predictions[\"y_score\"])\n",
    "        aucroc = skmetrics.roc_auc_score(self.y_true, self.predictions[\"y_score\"])\n",
    "        recall = skmetrics.recall_score(self.y_true, self.predictions[\"y_pred\"])\n",
    "        precision = skmetrics.precision_score(self.y_true, self.predictions[\"y_pred\"])\n",
    "        f_score = skmetrics.f1_score(self.y_true, self.predictions[\"y_pred\"])\n",
    "        acc = skmetrics.accuracy_score(self.y_true, self.predictions[\"y_pred\"])\n",
    "        \n",
    "        # EER\n",
    "        fpr, tpr, threshold = skmetrics.roc_curve(self.y_true, self.predictions[\"y_score\"], pos_label=1)\n",
    "        fnr = 1 - tpr\n",
    "        eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "        eer = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "\n",
    "        prefix = label + \"_\" if add_prefix and len(label) > 0 else \"\"\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"\"\"Performance metrics - {label}:\n",
    "                Total samples: {len(self.y_true)}\n",
    "                Positive samples: {np.sum(self.y_true)}\n",
    "                Positive proportion: {np.mean(self.y_true)}\n",
    "                ROC AUC: {aucroc:.3f}\n",
    "                Average Precision (PR AUC): {ap_score:.3f}\n",
    "                EER: {eer:.3f} (with threshold: {eer_threshold})\n",
    "\n",
    "                With classification threshold = {self.threshold}:\n",
    "                Precision: {precision:.3f};\n",
    "                Recall: {recall:.3f};\n",
    "                F-score: {f_score:.3f};\n",
    "                Accuracy: {acc:.3f}\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "        results_dict = {\n",
    "            f\"{prefix}aucpr\": [ap_score],\n",
    "            f\"{prefix}aucroc\": [aucroc],\n",
    "            f\"{prefix}precision\": [precision],\n",
    "            f\"{prefix}recall\": [recall],\n",
    "            f\"{prefix}f_score\": [f_score],\n",
    "            f\"{prefix}accuracy\": [acc],\n",
    "            f\"{prefix}eer\": [eer]\n",
    "        }\n",
    "\n",
    "        results_dataframe = pd.DataFrame(results_dict)\n",
    "\n",
    "        return results_dataframe\n",
    "\n",
    "    def set_threshold(self, new_threshold: float):\n",
    "        \"\"\"\n",
    "        Set the threshold value for the object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        new_threshold : float\n",
    "            The new threshold value to be set.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If the new threshold is less than 0 or greater than 1.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        This method updates the object's threshold value.\n",
    "\n",
    "        \"\"\"\n",
    "        if new_threshold < 0 or new_threshold > 1:\n",
    "            raise ValueError(\"The threshold value must be between 0 and 1\")\n",
    "\n",
    "        self.threshold = new_threshold\n",
    "        self.predictions = self.get_binary_predictions()\n",
    "\n",
    "    def get_best_threshold(\n",
    "        self,\n",
    "        step: float = 0.01,\n",
    "        metric: str = \"f_score\",\n",
    "        aux_condition: dict = None,\n",
    "        verbose: bool = True,\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Get the best threshold value based on a given metric.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        step : float, optional\n",
    "            Step size for threshold iteration (default is 0.01).\n",
    "\n",
    "        metric : str, optional\n",
    "            The evaluation metric to optimize for.\n",
    "            Supported metrics: \"precision\", \"recall\", \"f_score\", \"accuracy\"\n",
    "            (default is \"f_score\").\n",
    "\n",
    "        aux_condition : dict, optional\n",
    "            A dictionary containing an auxiliary metric and a floor value.\n",
    "            If specified, the best threshold will be the one that maximizes the\n",
    "            specified metric, while the auxiliary metric is greater than or equal\n",
    "            to the specified floor value.\n",
    "            Supported metrics: \"precision\", \"recall\", \"f_score\", \"accuracy\"\n",
    "            (default is None).\n",
    "\n",
    "        verbose : bool, optional\n",
    "            If True, print the best metric and threshold values (default is True).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The best threshold value based on the specified metric.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        This method iterates over threshold values and returns the one\n",
    "        that maximizes the specified evaluation metric.\n",
    "        \"\"\"\n",
    "        assert step < 1\n",
    "        assert metric in [\"precision\", \"recall\", \"f_score\", \"accuracy\"]\n",
    "        if aux_condition is not None:\n",
    "            assert aux_condition[\"metric\"] in [\"precision\", \"recall\", \"f_score\", \"accuracy\"]\n",
    "            assert aux_condition[\"floor\"] is not None\n",
    "            aux_metric = aux_condition[\"metric\"]\n",
    "            aux_metric_floor = aux_condition[\"floor\"]\n",
    "\n",
    "        current_threshold = self.threshold\n",
    "        best_metric = 0\n",
    "        best_threshold = 0\n",
    "        for i in range(int(1 / step) + 1):\n",
    "            candidate_threshold = i * step\n",
    "\n",
    "            self.set_threshold(candidate_threshold)\n",
    "            threshold_results = self.evaluate_binary_performance(verbose=False)\n",
    "\n",
    "            candidate_metric = threshold_results[metric].values[0]\n",
    "            if best_metric < candidate_metric:\n",
    "                if aux_condition is not None:\n",
    "                    if threshold_results[aux_metric].values[0] >= aux_metric_floor:\n",
    "                        best_metric = candidate_metric\n",
    "                        best_threshold = candidate_threshold\n",
    "                else:\n",
    "                    best_metric = candidate_metric\n",
    "                    best_threshold = candidate_threshold\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"best_{metric}: {best_metric}\")\n",
    "            print(f\"best_threshold: {best_threshold}\")\n",
    "\n",
    "        self.set_threshold(current_threshold)\n",
    "\n",
    "        return best_threshold\n",
    "\n",
    "    def plot_confusion_matrix(self, label: str = None, figsize: Tuple = (20, 5)):\n",
    "        \"\"\"\n",
    "        Plot confusion matrices with different normalization criteria.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        label : str, optional\n",
    "            A label for the confusion matrix.\n",
    "        figsize : Tuple, optional\n",
    "            Figure size.\n",
    "        \"\"\"\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        plt.suptitle(\n",
    "            f\"Confusion Matrices - {label}, threshold = {self.threshold:.3f}\",\n",
    "            fontsize=24,\n",
    "        )\n",
    "        colorbar = False\n",
    "        cmap = \"YlGnBu\"\n",
    "\n",
    "        # confusion matrix with totals\n",
    "        axes = plt.subplot(1, 4, 1)\n",
    "        plt.title(\"Totals\")\n",
    "        skmetrics.ConfusionMatrixDisplay.from_predictions(\n",
    "            self.y_true, self.predictions[\"y_pred\"], colorbar=colorbar, cmap=cmap, ax=axes\n",
    "        )\n",
    "\n",
    "        # confusion matrix normalized by total samples\n",
    "        axes = plt.subplot(1, 4, 2)\n",
    "        plt.title(\"Normalized by total\")\n",
    "        skmetrics.ConfusionMatrixDisplay.from_predictions(\n",
    "            self.y_true,\n",
    "            self.predictions[\"y_pred\"],\n",
    "            normalize=\"all\",\n",
    "            colorbar=colorbar,\n",
    "            cmap=cmap,\n",
    "            ax=axes,\n",
    "        )\n",
    "\n",
    "        # confusion matrix normalized by ground truth\n",
    "        axes = plt.subplot(1, 4, 3)\n",
    "        plt.title(\"Normalized by ground truth (rows add to 1)\")\n",
    "        skmetrics.ConfusionMatrixDisplay.from_predictions(\n",
    "            self.y_true,\n",
    "            self.predictions[\"y_pred\"],\n",
    "            normalize=\"true\",\n",
    "            colorbar=colorbar,\n",
    "            cmap=cmap,\n",
    "            ax=axes,\n",
    "        )\n",
    "\n",
    "        # confusion matrix normalized by prediction\n",
    "        axes = plt.subplot(1, 4, 4)\n",
    "        plt.title(\"Normalized by prediction (columns add to 1)\")\n",
    "        skmetrics.ConfusionMatrixDisplay.from_predictions(\n",
    "            self.y_true,\n",
    "            self.predictions[\"y_pred\"],\n",
    "            normalize=\"pred\",\n",
    "            colorbar=colorbar,\n",
    "            cmap=cmap,\n",
    "            ax=axes,\n",
    "        )\n",
    "\n",
    "        return fig\n",
    "\n",
    "    def plot_classification_performance(\n",
    "        self, label: str = None, figsize: Tuple = (14, 6), add_annotation_values: bool = True\n",
    "    ) -> plt.figure:\n",
    "        \"\"\"\n",
    "        Plot classification performance curves.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        label : str, optional\n",
    "            A label for the plots.\n",
    "        figsize : Tuple, optional\n",
    "            Figure size, specified as (width, height).\n",
    "        add_annotation_values : bool, optional\n",
    "            Whether to add annotation values to the curves.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        fig : matplotlib.figure.Figure\n",
    "            The matplotlib Figure containing the classification performance plots.\n",
    "        \"\"\"\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        plt.suptitle(f\"Classification Performance Curves ({label})\", fontsize=24)\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"ROC curve\")\n",
    "        fpr, tpr, _ = skmetrics.roc_curve(self.y_true, self.predictions[\"y_score\"])\n",
    "        aucroc_score = skmetrics.roc_auc_score(self.y_true, self.predictions[\"y_score\"])\n",
    "        plt.plot(fpr, tpr, marker=\".\", label=f\"Classifier (AUC = {aucroc_score})\")\n",
    "        plt.legend()\n",
    "        if add_annotation_values:\n",
    "            for x_value, y_value in zip(fpr, tpr):\n",
    "                label = f\"{y_value:.2f}\"\n",
    "                plt.annotate(\n",
    "                    label,  # this is the text\n",
    "                    (x_value, y_value),  # these are the coordinates to position the label\n",
    "                    textcoords=\"offset points\",  # how to position the text\n",
    "                    xytext=(0, 10),  # distance from text to points (x,y)\n",
    "                    ha=\"center\",\n",
    "                )  # horizontal alignment can be left, right or center\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"Precision/Recall curve\")\n",
    "        precision, recall, _ = skmetrics.precision_recall_curve(\n",
    "            self.y_true, self.predictions[\"y_score\"]\n",
    "        )\n",
    "        ap_score = skmetrics.average_precision_score(self.y_true, self.predictions[\"y_score\"])\n",
    "        plt.plot(recall, precision, marker=\".\", label=f\"Classifier (AP = {ap_score})\")\n",
    "        plt.legend()\n",
    "        if add_annotation_values:\n",
    "            for x_value, y_value in zip(precision, recall):\n",
    "                label = f\"{y_value:.2f}\"\n",
    "                plt.annotate(\n",
    "                    label,  # this is the text\n",
    "                    (x_value, y_value),  # these are the coordinates to position the label\n",
    "                    textcoords=\"offset points\",  # how to position the text\n",
    "                    xytext=(0, 10),  # distance from text to points (x,y)\n",
    "                    ha=\"center\",\n",
    "                )  # horizontal alignment can be left, right or center\n",
    "\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genSpoof_list(dir_meta, is_train=False, is_eval=False):\n",
    "    d_meta = {}\n",
    "    file_list = []\n",
    "    with open(dir_meta, \"r\") as f:\n",
    "        l_meta = f.readlines()\n",
    "\n",
    "    if is_train:\n",
    "        for line in l_meta:\n",
    "            _, key, _, _, label = line.strip().split()\n",
    "\n",
    "            file_list.append(key)\n",
    "            d_meta[key] = 1 if label == \"bonafide\" else 0\n",
    "        return d_meta, file_list\n",
    "\n",
    "    elif is_eval:\n",
    "        for line in l_meta:\n",
    "            key, score = line.strip().split()\n",
    "            file_list.append(key)\n",
    "            d_meta[key] = score\n",
    "        return d_meta, file_list\n",
    "    else:\n",
    "        for line in l_meta:\n",
    "            _, key, _, _, _, label, _, _, _, _, _, _, _ = line.strip().split()\n",
    "\n",
    "            file_list.append(key)\n",
    "            d_meta[key] = 1 if label == \"bonafide\" else 0\n",
    "        return d_meta, file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "true, files = genSpoof_list('/mnt/f/downloads/avs/trial_metadata.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, files_pred = genSpoof_list('eval.out', is_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DF_E_2000011</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DF_E_2000013</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DF_E_2000024</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DF_E_2000026</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DF_E_2000027</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DF_E_4999945</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DF_E_4999962</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DF_E_4999964</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DF_E_4999980</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DF_E_4999993</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>611829 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              label\n",
       "DF_E_2000011      0\n",
       "DF_E_2000013      0\n",
       "DF_E_2000024      0\n",
       "DF_E_2000026      0\n",
       "DF_E_2000027      0\n",
       "...             ...\n",
       "DF_E_4999945      0\n",
       "DF_E_4999962      0\n",
       "DF_E_4999964      0\n",
       "DF_E_4999980      0\n",
       "DF_E_4999993      0\n",
       "\n",
       "[611829 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "true_df = pd.DataFrame.from_dict(true, columns=['label'], orient='index')\n",
    "true_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DF_E_2000011</th>\n",
       "      <td>9.781375410966575e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DF_E_2000013</th>\n",
       "      <td>0.00038615192170254886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DF_E_2000024</th>\n",
       "      <td>6.89525450070505e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DF_E_2000026</th>\n",
       "      <td>5.140700523043051e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DF_E_2000027</th>\n",
       "      <td>3.5635108361020684e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DF_E_4570780</th>\n",
       "      <td>0.0004935489851050079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DF_E_4570782</th>\n",
       "      <td>0.00013525458052754402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DF_E_4570784</th>\n",
       "      <td>0.006551133468747139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DF_E_4570792</th>\n",
       "      <td>1.1531212606996633e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DF_E_4570793</th>\n",
       "      <td>0.00037851594970561564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>524288 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               score\n",
       "DF_E_2000011   9.781375410966575e-05\n",
       "DF_E_2000013  0.00038615192170254886\n",
       "DF_E_2000024    6.89525450070505e-06\n",
       "DF_E_2000026   5.140700523043051e-06\n",
       "DF_E_2000027  3.5635108361020684e-05\n",
       "...                              ...\n",
       "DF_E_4570780   0.0004935489851050079\n",
       "DF_E_4570782  0.00013525458052754402\n",
       "DF_E_4570784    0.006551133468747139\n",
       "DF_E_4570792  1.1531212606996633e-07\n",
       "DF_E_4570793  0.00037851594970561564\n",
       "\n",
       "[524288 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "preds_df = pd.DataFrame.from_dict(preds, columns=['score'], orient='index')\n",
    "preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(524288, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare = true_df.merge(preds_df, left_index=True, right_index=True)\n",
    "compare['score'] = compare['score'].astype(float)\n",
    "compare['label'] = compare['label'].astype(int)\n",
    "compare.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance metrics - :\n",
      "                Total samples: 524288\n",
      "                Positive samples: 19393\n",
      "                Positive proportion: 0.03698921203613281\n",
      "                ROC AUC: 0.777\n",
      "                Average Precision (PR AUC): 0.283\n",
      "                EER: 0.290 (with threshold: 0.0002053024509223178)\n",
      "\n",
      "                With classification threshold = 0.5:\n",
      "                Precision: 0.278;\n",
      "                Recall: 0.591;\n",
      "                F-score: 0.379;\n",
      "                Accuracy: 0.928\n",
      "                \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aucpr</th>\n",
       "      <th>aucroc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>eer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.283406</td>\n",
       "      <td>0.777305</td>\n",
       "      <td>0.27848</td>\n",
       "      <td>0.590935</td>\n",
       "      <td>0.378561</td>\n",
       "      <td>0.928236</td>\n",
       "      <td>0.289991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      aucpr    aucroc  precision    recall   f_score  accuracy       eer\n",
       "0  0.283406  0.777305    0.27848  0.590935  0.378561  0.928236  0.289991"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = BinaryPerformance(compare['label'], compare['score'])\n",
    "evaluator.evaluate_binary_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
