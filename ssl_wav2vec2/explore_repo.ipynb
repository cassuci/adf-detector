{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "import yaml\n",
    "from data_utils_SSL import genSpoof_list,Dataset_ASVspoof2019_train,Dataset_ASVspoof2021_eval\n",
    "from model import Model\n",
    "from tensorboardX import SummaryWriter\n",
    "from core_scripts.startup_config import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Hemlata Tak\"\n",
    "__email__ = \"tak@eurecom.fr\"\n",
    "\n",
    "\n",
    "def evaluate_accuracy(dev_loader, model, device):\n",
    "    val_loss = 0.0\n",
    "    num_total = 0.0\n",
    "    model.eval()\n",
    "    weight = torch.FloatTensor([0.1, 0.9]).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "    for batch_x, batch_y in dev_loader:\n",
    "        batch_size = batch_x.size(0)\n",
    "        num_total += batch_size\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.view(-1).type(torch.int64).to(device)\n",
    "        batch_out = model(batch_x)\n",
    "\n",
    "        batch_loss = criterion(batch_out, batch_y)\n",
    "        val_loss += batch_loss.item() * batch_size\n",
    "\n",
    "    val_loss /= num_total\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def produce_evaluation_file(dataset, model, device, save_path):\n",
    "    data_loader = DataLoader(dataset, batch_size=10, shuffle=False, drop_last=False)\n",
    "    num_correct = 0.0\n",
    "    num_total = 0.0\n",
    "    model.eval()\n",
    "\n",
    "    fname_list = []\n",
    "    key_list = []\n",
    "    score_list = []\n",
    "\n",
    "    for batch_x, utt_id in data_loader:\n",
    "        fname_list = []\n",
    "        score_list = []\n",
    "        batch_size = batch_x.size(0)\n",
    "        batch_x = batch_x.to(device)\n",
    "\n",
    "        batch_out = model(batch_x)\n",
    "\n",
    "        batch_score = (batch_out[:, 1]).data.cpu().numpy().ravel()\n",
    "        # add outputs\n",
    "        fname_list.extend(utt_id)\n",
    "        score_list.extend(batch_score.tolist())\n",
    "\n",
    "        with open(save_path, \"a+\") as fh:\n",
    "            for f, cm in zip(fname_list, score_list):\n",
    "                fh.write(\"{} {}\\n\".format(f, cm))\n",
    "        fh.close()\n",
    "    print(\"Scores saved to {}\".format(save_path))\n",
    "\n",
    "\n",
    "def train_epoch(train_loader, model, lr, optim, device):\n",
    "    running_loss = 0\n",
    "\n",
    "    num_total = 0.0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # set objective (Loss) functions\n",
    "    weight = torch.FloatTensor([0.1, 0.9]).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_size = batch_x.size(0)\n",
    "        num_total += batch_size\n",
    "\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.view(-1).type(torch.int64).to(device)\n",
    "        batch_out = model(batch_x)\n",
    "\n",
    "        batch_loss = criterion(batch_out, batch_y)\n",
    "\n",
    "        running_loss += batch_loss.item() * batch_size\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    running_loss /= num_total\n",
    "\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the argument parser\n",
    "parser = argparse.ArgumentParser(description='ASVspoof2021 baseline system')\n",
    "\n",
    "# Dataset\n",
    "parser.add_argument('--database_path', type=str, default='/mnt/f/downloads/avs/DF/', help='Change this to user\\'s full directory address of LA database (ASVspoof2019- for training & development (used as validation), ASVspoof2021 for evaluation scores). We assume that all three ASVspoof 2019 LA train, LA dev, and ASVspoof2021 LA eval data folders are in the same database_path directory.')\n",
    "parser.add_argument('--protocols_path', type=str, default='/mnt/f/downloads/avs/protocols_path/', help='Change with path to user\\'s LA database protocols directory address')\n",
    "\n",
    "# Hyperparameters\n",
    "parser.add_argument('--batch_size', type=int, default=14)\n",
    "parser.add_argument('--num_epochs', type=int, default=100)\n",
    "parser.add_argument('--lr', type=float, default=0.000001)\n",
    "parser.add_argument('--weight_decay', type=float, default=0.0001)\n",
    "parser.add_argument('--loss', type=str, default='weighted_CCE')\n",
    "\n",
    "# Model\n",
    "parser.add_argument('--seed', type=int, default=1234, help='random seed (default: 1234)')\n",
    "parser.add_argument('--model_path', type=str, default=None, help='Model checkpoint')\n",
    "parser.add_argument('--comment', type=str, default=None, help='Comment to describe the saved model')\n",
    "\n",
    "# Auxiliary arguments\n",
    "parser.add_argument('--track', type=str, default='LA', choices=['LA', 'PA', 'DF'], help='LA/PA/DF')\n",
    "parser.add_argument('--eval_output', type=str, default=None, help='Path to save the evaluation result')\n",
    "parser.add_argument('--eval', action='store_true', default=False, help='eval mode')\n",
    "parser.add_argument('--is_eval', action='store_true', default=False, help='eval database')\n",
    "parser.add_argument('--eval_part', type=int, default=0)\n",
    "\n",
    "# Backend options\n",
    "parser.add_argument('--cudnn-deterministic-toggle', action='store_false', default=True, help='use cudnn-deterministic? (default true)')\n",
    "parser.add_argument('--cudnn-benchmark-toggle', action='store_true', default=False, help='use cudnn-benchmark? (default false)')\n",
    "\n",
    "# Rawboost data augmentation\n",
    "parser.add_argument('--algo', type=int, default=5, help='Rawboost algos descriptions. 0: No augmentation, 1: LnL_convolutive_noise, 2: ISD_additive_noise, 3: SSI_additive_noise, 4: series algo (1+2+3), 5: series algo (1+2), 6: series algo (1+3), 7: series algo(2+3), 8: parallel algo(1,2) [default=5]')\n",
    "\n",
    "# LnL_convolutive_noise parameters\n",
    "parser.add_argument('--nBands', type=int, default=5, help='number of notch filters. The higher the number of bands, the more aggressive the distortions are. [default=5]')\n",
    "parser.add_argument('--minF', type=int, default=20, help='minimum center frequency [Hz] of notch filter. [default=20]')\n",
    "parser.add_argument('--maxF', type=int, default=8000, help='maximum center frequency [Hz] (<sr/2) of notch filter. [default=8000]')\n",
    "parser.add_argument('--minBW', type=int, default=100, help='minimum width [Hz] of filter. [default=100]')\n",
    "parser.add_argument('--maxBW', type=int, default=1000, help='maximum width [Hz] of filter. [default=1000]')\n",
    "parser.add_argument('--minCoeff', type=int, default=10, help='minimum filter coefficients. More filter coefficients mean a more ideal filter slope. [default=10]')\n",
    "parser.add_argument('--maxCoeff', type=int, default=100, help='maximum filter coefficients. More filter coefficients mean a more ideal filter slope. [default=100]')\n",
    "parser.add_argument('--minG', type=int, default=0, help='minimum gain factor of the linear component. [default=0]')\n",
    "parser.add_argument('--maxG', type=int, default=0, help='maximum gain factor of the linear component. [default=0]')\n",
    "parser.add_argument('--minBiasLinNonLin', type=int, default=5, help='minimum gain difference between linear and non-linear components. [default=5]')\n",
    "parser.add_argument('--maxBiasLinNonLin', type=int, default=20, help='maximum gain difference between linear and non-linear components. [default=20]')\n",
    "parser.add_argument('--N_f', type=int, default=5, help='order of the (non-)linearity where N_f=1 refers only to linear components. [default=5]')\n",
    "\n",
    "# ISD_additive_noise parameters\n",
    "parser.add_argument('--P', type=int, default=10, help='Maximum number of uniformly distributed samples in [%]. [default=10]')\n",
    "parser.add_argument('--g_sd', type=int, default=2, help='gain parameters > 0. [default=2]')\n",
    "\n",
    "# SSI_additive_noise parameters\n",
    "parser.add_argument('--SNRmin', type=int, default=10, help='Minimum SNR value for colored additive noise. [default=10]')\n",
    "parser.add_argument('--SNRmax', type=int, default=40, help='Maximum SNR value for colored additive noise. [default=40]')\n",
    "\n",
    "if not os.path.exists('models'):\n",
    "    os.mkdir('models')\n",
    "\n",
    "# Parse the arguments and store them in the \"args\" variable\n",
    "args = parser.parse_args(\"--track=LA --lr=0.000001 --batch_size=14 --loss=WCE\".split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(database_path='/mnt/f/downloads/avs/DF/', protocols_path='/mnt/f/downloads/avs/protocols_path/', batch_size=14, num_epochs=100, lr=1e-06, weight_decay=0.0001, loss='WCE', seed=1234, model_path=None, comment=None, track='LA', eval_output=None, eval=False, is_eval=False, eval_part=0, cudnn_deterministic_toggle=True, cudnn_benchmark_toggle=False, algo=5, nBands=5, minF=20, maxF=8000, minBW=100, maxBW=1000, minCoeff=10, maxCoeff=100, minG=0, maxG=0, minBiasLinNonLin=5, maxBiasLinNonLin=20, N_f=5, P=10, g_sd=2, SNRmin=10, SNRmax=40)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make experiment reproducible\n",
    "set_random_seed(args.seed, args)\n",
    "\n",
    "track = args.track\n",
    "\n",
    "assert track in [\"LA\", \"PA\", \"DF\"], \"Invalid track given\"\n",
    "\n",
    "# database\n",
    "prefix = \"ASVspoof_{}\".format(track)\n",
    "prefix_2019 = \"ASVspoof2019.{}\".format(track)\n",
    "prefix_2021 = \"ASVspoof2021.{}\".format(track)\n",
    "\n",
    "# define model saving path\n",
    "model_tag = \"model_{}_{}_{}_{}_{}\".format(\n",
    "    track, args.loss, args.num_epochs, args.batch_size, args.lr\n",
    ")\n",
    "if args.comment:\n",
    "    model_tag = model_tag + \"_{}\".format(args.comment)\n",
    "model_save_path = os.path.join(\"models\", model_tag)\n",
    "\n",
    "# set model save directory\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.mkdir(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "nb_params: 317837834\n"
     ]
    }
   ],
   "source": [
    "# GPU device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "model = Model(args, device)\n",
    "nb_params = sum([param.view(-1).size()[0] for param in model.parameters()])\n",
    "model = model.to(device)\n",
    "print(\"nb_params:\", nb_params)\n",
    "\n",
    "# set Adam optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.model_path:\n",
    "    model.load_state_dict(torch.load(args.model_path, map_location=device))\n",
    "    print(\"Model loaded : {}\".format(args.model_path))\n",
    "\n",
    "\n",
    "# evaluation\n",
    "if args.eval:\n",
    "    file_eval = genSpoof_list(\n",
    "        dir_meta=os.path.join(\n",
    "            args.protocols_path\n",
    "            + \"{}_cm_protocols/{}.cm.eval.trl.txt\".format(prefix, prefix_2021)\n",
    "        ),\n",
    "        is_train=False,\n",
    "        is_eval=True,\n",
    "    )\n",
    "    print(\"no. of eval trials\", len(file_eval))\n",
    "    eval_set = Dataset_ASVspoof2021_eval(\n",
    "        list_IDs=file_eval,\n",
    "        base_dir=os.path.join(\n",
    "            args.database_path + \"ASVspoof2021_{}_eval/\".format(args.track)\n",
    "        ),\n",
    "    )\n",
    "    produce_evaluation_file(eval_set, model, device, args.eval_output)\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of training trials 25380\n",
      "no. of validation trials 24844\n"
     ]
    }
   ],
   "source": [
    "# define train dataloader\n",
    "d_label_trn, file_train = genSpoof_list(\n",
    "    dir_meta=os.path.join(\n",
    "        args.protocols_path\n",
    "        + \"{}_cm_protocols/{}.cm.train.trn.txt\".format(prefix, prefix_2019)\n",
    "    ),\n",
    "    is_train=True,\n",
    "    is_eval=False,\n",
    ")\n",
    "\n",
    "print(\"no. of training trials\", len(file_train))\n",
    "\n",
    "train_set = Dataset_ASVspoof2019_train(\n",
    "    args,\n",
    "    list_IDs=file_train,\n",
    "    labels=d_label_trn,\n",
    "    base_dir=os.path.join(\n",
    "        args.database_path\n",
    "        + \"{}_{}_train/\".format(prefix_2019.split(\".\")[0], args.track)\n",
    "    ),\n",
    "    algo=args.algo,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set, batch_size=args.batch_size, num_workers=8, shuffle=True, drop_last=True\n",
    ")\n",
    "\n",
    "del train_set, d_label_trn\n",
    "\n",
    "\n",
    "# define validation dataloader\n",
    "\n",
    "d_label_dev, file_dev = genSpoof_list(\n",
    "    dir_meta=os.path.join(\n",
    "        args.protocols_path\n",
    "        + \"{}_cm_protocols/{}.cm.dev.trl.txt\".format(prefix, prefix_2019)\n",
    "    ),\n",
    "    is_train=False,\n",
    "    is_eval=False,\n",
    ")\n",
    "\n",
    "print(\"no. of validation trials\", len(file_dev))\n",
    "\n",
    "dev_set = Dataset_ASVspoof2019_train(\n",
    "    args,\n",
    "    list_IDs=file_dev,\n",
    "    labels=d_label_dev,\n",
    "    base_dir=os.path.join(\n",
    "        args.database_path + \"{}_{}_dev/\".format(prefix_2019.split(\".\")[0], args.track)\n",
    "    ),\n",
    "    algo=args.algo,\n",
    ")\n",
    "dev_loader = DataLoader(\n",
    "    dev_set, batch_size=args.batch_size, num_workers=8, shuffle=False\n",
    ")\n",
    "del dev_set, d_label_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation\n",
    "num_epochs = args.num_epochs\n",
    "writer = SummaryWriter(\"logs/{}\".format(model_tag))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = train_epoch(train_loader, model, args.lr, optimizer, device)\n",
    "    val_loss = evaluate_accuracy(dev_loader, model, device)\n",
    "    writer.add_scalar(\"val_loss\", val_loss, epoch)\n",
    "    writer.add_scalar(\"loss\", running_loss, epoch)\n",
    "    print(\"\\n{} - {} - {} \".format(epoch, running_loss, val_loss))\n",
    "    torch.save(\n",
    "        model.state_dict(), os.path.join(model_save_path, \"epoch_{}.pth\".format(epoch))\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
