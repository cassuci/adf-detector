{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6638f7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import logging\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"FALSE\"\n",
    "\n",
    "from scipy.io import wavfile\n",
    "\n",
    "\n",
    "# Configure the logging settings\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c0a27df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Load, preprocess, and pad audio data\n",
    "def preprocess_audio(audio_file, target_length=300):\n",
    "    #logging.info(f'Processing audio file: {audio_file}')\n",
    "    audio, _ = librosa.load(audio_file, sr=22050)\n",
    "    audio_mfcc = librosa.feature.mfcc(y=audio, sr=22050)\n",
    "\n",
    "    # Calculate the current length of audio data\n",
    "    current_length = audio_mfcc.shape[1]\n",
    "\n",
    "    if current_length < target_length:\n",
    "        # If the audio is shorter than the target length, pad it\n",
    "        pad_width = target_length - current_length\n",
    "        audio_mfcc = np.pad(audio_mfcc, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        # If the audio is longer, truncate it\n",
    "        audio_mfcc = audio_mfcc[:, :target_length]\n",
    "\n",
    "    return audio_mfcc\n",
    "\n",
    "# Generator function for lazy loading of audio data\n",
    "def audio_data_generator(audio_files):\n",
    "    for audio_file in audio_files:\n",
    "        yield preprocess_audio(audio_file)\n",
    "\n",
    "# Load a list of audio files\n",
    "audio_files_df = pd.read_csv('data_files_summary.csv')\n",
    "audio_files = audio_files_df['full_path'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ada21806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a generator to load and preprocess audio data on-the-fly\n",
    "data_generator = audio_data_generator(audio_files)\n",
    "data_iterator = iter(data_generator)  # Convert the generator to an iterator\n",
    "\n",
    "# To train an autoencoder, you need target data, which is the same as the input data\n",
    "# So, use the same data for both input and target\n",
    "X_train = np.array(list(data_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4366cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the autoencoder architecture\n",
    "encoding_dim = 1024\n",
    "\n",
    "input_audio = tf.keras.layers.Input(shape=(20, 300))  # Variable-length input\n",
    "flattened_input = tf.keras.layers.Flatten()(input_audio)  # Flatten the input\n",
    "encoded = tf.keras.layers.Dense(encoding_dim, activation='relu')(flattened_input)\n",
    "# Define the decoder with the final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e84dbbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = tf.keras.layers.Dense(20 * 300, activation='linear')(encoded)\n",
    "decoded = tf.keras.layers.Reshape((20, 300))(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75fe082e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-19 18:42:17,274 - root - INFO - Autoencoder model built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 20, 300)]         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 6000)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1024)              6145024   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6000)              6150000   \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 20, 300)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,295,024\n",
      "Trainable params: 12,295,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder = tf.keras.models.Model(input_audio, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "logging.info('Autoencoder model built.')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600094be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-19 18:42:17,300 - root - INFO - Training the autoencoder...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "197/197 [==============================] - 7s 32ms/step - loss: 752.8079\n",
      "Epoch 2/300\n",
      "197/197 [==============================] - 6s 32ms/step - loss: 399.1979\n",
      "Epoch 3/300\n",
      "197/197 [==============================] - 6s 33ms/step - loss: 305.4923\n",
      "Epoch 4/300\n",
      "197/197 [==============================] - 6s 32ms/step - loss: 238.5483\n",
      "Epoch 5/300\n",
      "197/197 [==============================] - 7s 33ms/step - loss: 201.2288\n",
      "Epoch 6/300\n",
      "197/197 [==============================] - 6s 31ms/step - loss: 178.9475\n",
      "Epoch 7/300\n",
      "197/197 [==============================] - 6s 31ms/step - loss: 168.5181\n",
      "Epoch 8/300\n",
      "197/197 [==============================] - 6s 32ms/step - loss: 152.0480\n",
      "Epoch 9/300\n",
      "197/197 [==============================] - 7s 34ms/step - loss: 148.1080\n",
      "Epoch 10/300\n",
      "197/197 [==============================] - 7s 34ms/step - loss: 137.6384\n",
      "Epoch 11/300\n",
      "197/197 [==============================] - 6s 32ms/step - loss: 131.5908\n",
      "Epoch 12/300\n",
      "197/197 [==============================] - 7s 33ms/step - loss: 133.0995\n",
      "Epoch 13/300\n",
      "197/197 [==============================] - 6s 32ms/step - loss: 120.0955\n",
      "Epoch 14/300\n",
      "197/197 [==============================] - 6s 32ms/step - loss: 119.4376\n",
      "Epoch 15/300\n",
      "197/197 [==============================] - 6s 32ms/step - loss: 117.5905\n",
      "Epoch 16/300\n",
      "197/197 [==============================] - 6s 32ms/step - loss: 118.9022\n",
      "Epoch 17/300\n",
      "197/197 [==============================] - 7s 38ms/step - loss: 116.1472\n",
      "Epoch 18/300\n",
      "197/197 [==============================] - 7s 35ms/step - loss: 113.9196\n",
      "Epoch 19/300\n",
      "197/197 [==============================] - 7s 38ms/step - loss: 107.3548\n",
      "Epoch 20/300\n",
      "197/197 [==============================] - 7s 34ms/step - loss: 108.0385\n",
      "Epoch 21/300\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 111.5367\n",
      "Epoch 22/300\n",
      "197/197 [==============================] - 7s 34ms/step - loss: 106.3681\n",
      "Epoch 23/300\n",
      "197/197 [==============================] - 7s 34ms/step - loss: 105.4031\n",
      "Epoch 24/300\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 105.3736\n",
      "Epoch 25/300\n",
      "197/197 [==============================] - 6s 33ms/step - loss: 107.8342\n",
      "Epoch 26/300\n",
      "197/197 [==============================] - 7s 34ms/step - loss: 104.0669\n",
      "Epoch 27/300\n",
      "197/197 [==============================] - 7s 35ms/step - loss: 101.5581\n",
      "Epoch 28/300\n",
      "197/197 [==============================] - 7s 35ms/step - loss: 107.7492\n",
      "Epoch 29/300\n",
      "197/197 [==============================] - 7s 34ms/step - loss: 105.1250\n",
      "Epoch 30/300\n",
      "197/197 [==============================] - 7s 34ms/step - loss: 104.3054\n",
      "Epoch 31/300\n",
      "197/197 [==============================] - 7s 35ms/step - loss: 102.3194\n",
      "Epoch 32/300\n",
      "197/197 [==============================] - 7s 34ms/step - loss: 103.7065\n",
      "Epoch 33/300\n",
      "197/197 [==============================] - 7s 35ms/step - loss: 102.4071\n",
      "Epoch 34/300\n",
      "197/197 [==============================] - 7s 35ms/step - loss: 101.9430\n",
      "Epoch 35/300\n",
      "197/197 [==============================] - 7s 33ms/step - loss: 103.2900\n",
      "Epoch 36/300\n",
      " 33/197 [====>.........................] - ETA: 5s - loss: 96.7588"
     ]
    }
   ],
   "source": [
    "# Step 3: Train the autoencoder using audio data\n",
    "logging.info('Training the autoencoder...')\n",
    "autoencoder.fit(X_train, X_train, epochs=300, batch_size=32)\n",
    "logging.info('Training complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff429cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 4: Stream audio files through the trained autoencoder and save the encoded audio as WAV\n",
    "def stream_audio_through_autoencoder(audio_file, autoencoder, output_path):\n",
    "    input_audio = preprocess_audio(audio_file)\n",
    "    encoded_audio = autoencoder.predict(np.expand_dims(input_audio, axis=0))\n",
    "    \n",
    "    # Inverse transform the encoded audio back to the waveform\n",
    "    decoded_audio = librosa.feature.inverse.mfcc_to_audio(encoded_audio[0], sr=22050)\n",
    "    \n",
    "    # Scale the audio data to the appropriate range\n",
    "    decoded_audio = (decoded_audio * np.iinfo(np.int16).max).astype(np.int16)\n",
    "    \n",
    "    # Save the decoded audio as a WAV file\n",
    "    wavfile.write(output_path, 22050, decoded_audio)\n",
    "\n",
    "# Specify the output path for the saved WAV file\n",
    "output_path = \"encoded_audio.wav\"\n",
    "\n",
    "# Process an audio file and save the encoded audio as a WAV\n",
    "stream_audio_through_autoencoder(\"sample-3s.wav\", autoencoder, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
