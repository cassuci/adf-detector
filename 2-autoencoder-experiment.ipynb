{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6638f7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import librosa\n",
    "import logging\n",
    "import os\n",
    "from scipy.io import wavfile\n",
    "\n",
    "#os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "# Configure the logging settings\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb6f2df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def audio_to_spectrogram(audio_file_path, max_length=6*22500, sr=22500):\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_file_path, sr=sr)\n",
    "    \n",
    "    if len(y) > max_length:\n",
    "        # If the audio is longer than the maximum length, cut it\n",
    "        y = y[:max_length]\n",
    "    elif len(y) < max_length:\n",
    "        # If the audio is shorter, pad it with zeros\n",
    "        pad_length = max_length - len(y)\n",
    "        y = np.pad(y, (0, pad_length), mode='constant')\n",
    "    \n",
    "    # Calculate the STFT\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "    \n",
    "    return D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c0a27df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator function for lazy loading of audio data\n",
    "def audio_data_generator(audio_files):\n",
    "    for audio_file in audio_files:\n",
    "        yield audio_to_spectrogram(audio_file)\n",
    "\n",
    "# Load a list of audio files\n",
    "audio_files_df = pd.read_csv('data_files_summary.csv')\n",
    "audio_files = audio_files_df['full_path'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd40cca1-e383-4352-b9ef-99ae0f929ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-61.096313 -55.76928  -54.597748 ... -80.       -80.       -80.      ]\n",
      " [-62.884403 -60.52305  -60.369137 ... -80.       -80.       -80.      ]\n",
      " [-68.10806  -78.98971  -80.       ... -80.       -80.       -80.      ]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n",
      "[[-61.27263  -56.059315 -55.41803  ... -80.       -80.       -80.      ]\n",
      " [-63.211212 -61.038136 -60.47317  ... -80.       -80.       -80.      ]\n",
      " [-68.78768  -80.       -73.29738  ... -80.       -80.       -80.      ]\n",
      " ...\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]\n",
      " [-80.       -80.       -80.       ... -80.       -80.       -80.      ]]\n"
     ]
    }
   ],
   "source": [
    "for audio_file in audio_files[:2]:\n",
    "    print(audio_to_spectrogram(audio_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ada21806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 47.4 s\n",
      "Wall time: 48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create a generator to load and preprocess audio data on-the-fly\n",
    "data_generator = audio_data_generator(audio_files[:4500])\n",
    "data_iterator = iter(data_generator)  # Convert the generator to an iterator\n",
    "\n",
    "# To train an autoencoder, you need target data, which is the same as the input data\n",
    "# So, use the same data for both input and target\n",
    "X_train = np.array(list(data_iterator))\n",
    "#X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56289d9a-9e32-4afe-854c-b2724b22da9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 1025, 264)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3984dc04-674c-4da5-b54f-070db6a58c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(4500, 1025, 264, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe37b25e-36f8-408e-bfe9-2501b077a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = ((X_train + 80)/80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93afcbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1025, 264, 1)]    0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 1025, 264, 128)    1280      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 513, 132, 128)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 513, 132, 32)      36896     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 257, 66, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 257, 66, 32)       9248      \n",
      "                                                                 \n",
      " up_sampling2d_2 (UpSamplin  (None, 514, 132, 32)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 514, 132, 128)     36992     \n",
      "                                                                 \n",
      " up_sampling2d_3 (UpSamplin  (None, 1028, 264, 128)    0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " cropping2d_1 (Cropping2D)   (None, 1025, 264, 128)    0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 1025, 264, 1)      1153      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85569 (334.25 KB)\n",
      "Trainable params: 85569 (334.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D, Normalization, Reshape, Dense, Flatten, Cropping2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (1025, 264, 1)  # You can adjust the input shape based on your data\n",
    "\n",
    "# Define the encoder\n",
    "input_layer = Input(shape=input_shape)\n",
    "x = input_layer\n",
    "\n",
    "x = Conv2D(128, kernel_size = (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Cropping2D(cropping=((3, 0), (0, 0)))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Compile the autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "# Print the model summary\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600094be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-20 00:55:13,570 - root - INFO - Training the autoencoder...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "  1/141 [..............................] - ETA: 3:17:23 - loss: 0.1916"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "# Step 3: Train the autoencoder using audio data\n",
    "logging.info('Training the autoencoder...')\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled, batch_size=32, epochs=300, callbacks=[early_stopping])\n",
    "logging.info('Training complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad03745-8c49-4ed8-bff5-8531aa909a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save('autoencoder_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff429cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf  # Soundfile library for saving the WAV file\n",
    "def spectrogram_to_audio(spectrogram, sr=22500, max_length=6*22500):\n",
    "    # Inverse operation to recover the magnitude spectrum\n",
    "    magnitude = librosa.db_to_amplitude(D)#, ref=np.max)\n",
    "    \n",
    "    # Inverse Short-Time Fourier Transform (iSTFT) to obtain the time-domain signal\n",
    "    y_reconstructed = librosa.istft(magnitude)\n",
    "\n",
    "    return y_reconstructed\n",
    "\n",
    "# Example usage\n",
    "spectrogram = audio_to_spectrogram(\"converted.wav\", sr=48000).reshape((1, 1025, 264, 1))\n",
    "spectrogram = (spectrogram+80)/80\n",
    "out_spectre = autoencoder.predict(spectrogram)\n",
    "out_spectre = (out_spectre*80)-80\n",
    "out_spectre = out_spectre.reshape((1025, 308))\n",
    "reconstructed_audio = spectrogram_to_audio(out_spectre, 22500) * 1e3\n",
    "sf.write(\"reconstructed_audio.wav\", np.ravel(reconstructed_audio), 22500)  # Adjust the sample rate as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3fe255-a745-4be0-ab94-4596bea2b701",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a6d6b1-177c-41bd-9882-10f9fc17bd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load('converted.wav', sr=22500, duration=112500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cee932-b66b-449f-b983-87a46ae89fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab235a92-a5e7-46cd-b296-c5e308cd28d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reconstructed_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a32eb57-d61e-49d3-ae6d-505955fa0fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(out_spectre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b27cf6-b4d8-4196-af0e-fa3a93348fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(spectrogram[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bfd2d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250c639c-ca8e-4a09-863d-2aa738d36d92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
