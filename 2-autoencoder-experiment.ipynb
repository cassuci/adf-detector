{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6638f7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import logging\n",
    "import os\n",
    "from scipy.io import wavfile\n",
    "import vae\n",
    "\n",
    "# Configure the logging settings\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df2e6085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available GPUs\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2c69e47",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (185336717.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 8\u001b[1;36m\u001b[0m\n\u001b[1;33m    else:\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "if len(physical_devices) > 0:\n",
    "    # Ensure TensorFlow uses the first GPU\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    with tf.device('/GPU:0'):\n",
    "        \n",
    "else:\n",
    "    print(\"No GPU devices found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c0a27df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Load, preprocess, and pad audio data\n",
    "def preprocess_audio(audio_file, target_length=300):\n",
    "    #logging.info(f'Processing audio file: {audio_file}')\n",
    "    audio, _ = librosa.load(audio_file, sr=22050)\n",
    "    audio_mfcc = librosa.feature.mfcc(y=audio, sr=22050)\n",
    "\n",
    "    # Calculate the current length of audio data\n",
    "    current_length = audio_mfcc.shape[1]\n",
    "\n",
    "    if current_length < target_length:\n",
    "        # If the audio is shorter than the target length, pad it\n",
    "        pad_width = target_length - current_length\n",
    "        audio_mfcc = np.pad(audio_mfcc, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        # If the audio is longer, truncate it\n",
    "        audio_mfcc = audio_mfcc[:, :target_length]\n",
    "\n",
    "    return audio_mfcc\n",
    "\n",
    "# Generator function for lazy loading of audio data\n",
    "def audio_data_generator(audio_files):\n",
    "    for audio_file in audio_files:\n",
    "        yield preprocess_audio(audio_file)\n",
    "\n",
    "# Load a list of audio files\n",
    "audio_files_df = pd.read_csv('data_files_summary.csv')\n",
    "audio_files = audio_files_df['full_path'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ada21806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a generator to load and preprocess audio data on-the-fly\n",
    "data_generator = audio_data_generator(audio_files)\n",
    "data_iterator = iter(data_generator)  # Convert the generator to an iterator\n",
    "\n",
    "# To train an autoencoder, you need target data, which is the same as the input data\n",
    "# So, use the same data for both input and target\n",
    "X_train = np.array(list(data_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b26f9fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Step 2: Define the autoencoder architecture\n",
    "input_audio = tf.keras.layers.Input(shape=(20, 300, 1))  # Input shape with one channel\n",
    "encoded = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(input_audio)\n",
    "encoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(encoded)\n",
    "encoded = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "encoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(encoded)\n",
    "# Flatten the encoded layer before using it in the LSTM\n",
    "encoded = tf.keras.layers.Flatten()(encoded)\n",
    "encoded = tf.keras.layers.Reshape((20, 75 * 8))(encoded)\n",
    "decoded = tf.keras.layers.LSTM(300, return_sequences=True)(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4366cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the decoded layer to match the original input shape\n",
    "decoded = tf.keras.layers.Reshape((20, 300, 1))(decoded)\n",
    "autoencoder = tf.keras.models.Model(input_audio, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7a5957b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py:562: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-19 20:13:03,147 - tensorflow - WARNING - From C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py:562: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'method' object has no attribute '_from_serialized'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m autoencoder \u001b[38;5;241m=\u001b[39m vae\u001b[38;5;241m.\u001b[39mVAE(\n\u001b[0;32m      2\u001b[0m         input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m      3\u001b[0m         conv_filters\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m         latent_space_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m      7\u001b[0m     )\n\u001b[1;32m----> 8\u001b[0m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\UNICAMP\\deepfake\\adf-detector\\vae.py:53\u001b[0m, in \u001b[0;36mVAE.compile\u001b[1;34m(self, learning_rate)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile\u001b[39m(\u001b[38;5;28mself\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m):\n\u001b[0;32m     52\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m Adam(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calculate_combined_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calculate_reconstruction_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calculate_kl_loss\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:477\u001b[0m, in \u001b[0;36mModel.compile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mget_graph()\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;66;03m# Save all metric attributes per output of the model.\u001b[39;00m\n\u001b[1;32m--> 477\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache_output_metric_attributes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweighted_metrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;66;03m# Set metric attributes on model.\u001b[39;00m\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_metric_attributes()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2010\u001b[0m, in \u001b[0;36mModel._cache_output_metric_attributes\u001b[1;34m(self, metrics, weighted_metrics)\u001b[0m\n\u001b[0;32m   2007\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2008\u001b[0m         output_shapes\u001b[38;5;241m.\u001b[39mappend(output\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mas_list())\n\u001b[0;32m   2009\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_per_output_metrics \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 2010\u001b[0m     \u001b[43mtraining_utils_v1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_per_output_metric_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2012\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2013\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2014\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_serialized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_serialized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2016\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2017\u001b[0m )\n\u001b[0;32m   2018\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_per_output_weighted_metrics \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2019\u001b[0m     training_utils_v1\u001b[38;5;241m.\u001b[39mcollect_per_output_metric_info(\n\u001b[0;32m   2020\u001b[0m         weighted_metrics,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2026\u001b[0m     )\n\u001b[0;32m   2027\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_utils_v1.py:1041\u001b[0m, in \u001b[0;36mcollect_per_output_metric_info\u001b[1;34m(metrics, output_names, output_shapes, loss_fns, from_serialized, is_weighted)\u001b[0m\n\u001b[0;32m   1037\u001b[0m metric_name \u001b[38;5;241m=\u001b[39m get_metric_name(metric, is_weighted)\n\u001b[0;32m   1038\u001b[0m metric_fn \u001b[38;5;241m=\u001b[39m get_metric_function(\n\u001b[0;32m   1039\u001b[0m     metric, output_shape\u001b[38;5;241m=\u001b[39moutput_shapes[i], loss_fn\u001b[38;5;241m=\u001b[39mloss_fns[i]\n\u001b[0;32m   1040\u001b[0m )\n\u001b[1;32m-> 1041\u001b[0m metric_fn\u001b[38;5;241m.\u001b[39m_from_serialized \u001b[38;5;241m=\u001b[39m from_serialized\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;66;03m# If the metric function is not stateful, we create a stateful\u001b[39;00m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;66;03m# version.\u001b[39;00m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(metric_fn, metrics_module\u001b[38;5;241m.\u001b[39mMetric):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'method' object has no attribute '_from_serialized'"
     ]
    }
   ],
   "source": [
    "autoencoder = vae.VAE(\n",
    "        input_shape=(20, 300, 1),\n",
    "        conv_filters=(32, 64, 64, 64),\n",
    "        conv_kernels=(3, 3, 3, 3),\n",
    "        conv_strides=(1, 2, 2, 1),\n",
    "        latent_space_dim=2\n",
    "    )\n",
    "autoencoder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "75fe082e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-19 20:10:20,873 - root - INFO - Autoencoder model built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 20, 300, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " encoder_conv_layer_1 (Conv2D)  (None, 20, 300, 32)  320         ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " encoder_relu_1 (ReLU)          (None, 20, 300, 32)  0           ['encoder_conv_layer_1[0][0]']   \n",
      "                                                                                                  \n",
      " encoder_bn_1 (BatchNormalizati  (None, 20, 300, 32)  128        ['encoder_relu_1[0][0]']         \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " encoder_conv_layer_2 (Conv2D)  (None, 10, 150, 64)  18496       ['encoder_bn_1[0][0]']           \n",
      "                                                                                                  \n",
      " encoder_relu_2 (ReLU)          (None, 10, 150, 64)  0           ['encoder_conv_layer_2[0][0]']   \n",
      "                                                                                                  \n",
      " encoder_bn_2 (BatchNormalizati  (None, 10, 150, 64)  256        ['encoder_relu_2[0][0]']         \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " encoder_conv_layer_3 (Conv2D)  (None, 5, 75, 64)    36928       ['encoder_bn_2[0][0]']           \n",
      "                                                                                                  \n",
      " encoder_relu_3 (ReLU)          (None, 5, 75, 64)    0           ['encoder_conv_layer_3[0][0]']   \n",
      "                                                                                                  \n",
      " encoder_bn_3 (BatchNormalizati  (None, 5, 75, 64)   256         ['encoder_relu_3[0][0]']         \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " encoder_conv_layer_4 (Conv2D)  (None, 5, 75, 64)    36928       ['encoder_bn_3[0][0]']           \n",
      "                                                                                                  \n",
      " encoder_relu_4 (ReLU)          (None, 5, 75, 64)    0           ['encoder_conv_layer_4[0][0]']   \n",
      "                                                                                                  \n",
      " encoder_bn_4 (BatchNormalizati  (None, 5, 75, 64)   256         ['encoder_relu_4[0][0]']         \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " flatten_9 (Flatten)            (None, 24000)        0           ['encoder_bn_4[0][0]']           \n",
      "                                                                                                  \n",
      " mu (Dense)                     (None, 2)            48002       ['flatten_9[0][0]']              \n",
      "                                                                                                  \n",
      " log_variance (Dense)           (None, 2)            48002       ['flatten_9[0][0]']              \n",
      "                                                                                                  \n",
      " encoder_output (Lambda)        (None, 2)            0           ['mu[0][0]',                     \n",
      "                                                                  'log_variance[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 189,572\n",
      "Trainable params: 189,124\n",
      "Non-trainable params: 448\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 2)]               0         \n",
      "                                                                 \n",
      " decoder_dense (Dense)       (None, 24000)             72000     \n",
      "                                                                 \n",
      " reshape_16 (Reshape)        (None, 5, 75, 64)         0         \n",
      "                                                                 \n",
      " decoder_conv_transpose_laye  (None, 5, 75, 64)        36928     \n",
      " r_1 (Conv2DTranspose)                                           \n",
      "                                                                 \n",
      " decoder_relu_1 (ReLU)       (None, 5, 75, 64)         0         \n",
      "                                                                 \n",
      " decoder_bn_1 (BatchNormaliz  (None, 5, 75, 64)        256       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " decoder_conv_transpose_laye  (None, 10, 150, 64)      36928     \n",
      " r_2 (Conv2DTranspose)                                           \n",
      "                                                                 \n",
      " decoder_relu_2 (ReLU)       (None, 10, 150, 64)       0         \n",
      "                                                                 \n",
      " decoder_bn_2 (BatchNormaliz  (None, 10, 150, 64)      256       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " decoder_conv_transpose_laye  (None, 20, 300, 64)      36928     \n",
      " r_3 (Conv2DTranspose)                                           \n",
      "                                                                 \n",
      " decoder_relu_3 (ReLU)       (None, 20, 300, 64)       0         \n",
      "                                                                 \n",
      " decoder_bn_3 (BatchNormaliz  (None, 20, 300, 64)      256       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " decoder_conv_transpose_laye  (None, 20, 300, 1)       577       \n",
      " r_4 (Conv2DTranspose)                                           \n",
      "                                                                 \n",
      " sigmoid_layer (Activation)  (None, 20, 300, 1)        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 184,129\n",
      "Trainable params: 183,745\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_input (InputLayer)  [(None, 20, 300, 1)]      0         \n",
      "                                                                 \n",
      " encoder (Functional)        (None, 2)                 189572    \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 20, 300, 1)        184129    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 373,701\n",
      "Trainable params: 372,869\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "logging.info('Autoencoder model built.')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "600094be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-19 20:09:34,048 - root - INFO - Training the autoencoder...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'method' object has no attribute '_from_serialized'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 3: Train the autoencoder using audio data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining the autoencoder...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m autoencoder\u001b[38;5;241m.\u001b[39mtrain(X_train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)\n\u001b[0;32m      5\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining complete.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\UNICAMP\\deepfake\\adf-detector\\vae.py:53\u001b[0m, in \u001b[0;36mVAE.compile\u001b[1;34m(self, learning_rate)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile\u001b[39m(\u001b[38;5;28mself\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m):\n\u001b[0;32m     52\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m Adam(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calculate_combined_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calculate_reconstruction_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calculate_kl_loss\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:477\u001b[0m, in \u001b[0;36mModel.compile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mget_graph()\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;66;03m# Save all metric attributes per output of the model.\u001b[39;00m\n\u001b[1;32m--> 477\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache_output_metric_attributes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweighted_metrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;66;03m# Set metric attributes on model.\u001b[39;00m\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_metric_attributes()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2010\u001b[0m, in \u001b[0;36mModel._cache_output_metric_attributes\u001b[1;34m(self, metrics, weighted_metrics)\u001b[0m\n\u001b[0;32m   2007\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2008\u001b[0m         output_shapes\u001b[38;5;241m.\u001b[39mappend(output\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mas_list())\n\u001b[0;32m   2009\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_per_output_metrics \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 2010\u001b[0m     \u001b[43mtraining_utils_v1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_per_output_metric_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2012\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2013\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2014\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_serialized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_serialized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2016\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2017\u001b[0m )\n\u001b[0;32m   2018\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_per_output_weighted_metrics \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2019\u001b[0m     training_utils_v1\u001b[38;5;241m.\u001b[39mcollect_per_output_metric_info(\n\u001b[0;32m   2020\u001b[0m         weighted_metrics,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2026\u001b[0m     )\n\u001b[0;32m   2027\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_utils_v1.py:1041\u001b[0m, in \u001b[0;36mcollect_per_output_metric_info\u001b[1;34m(metrics, output_names, output_shapes, loss_fns, from_serialized, is_weighted)\u001b[0m\n\u001b[0;32m   1037\u001b[0m metric_name \u001b[38;5;241m=\u001b[39m get_metric_name(metric, is_weighted)\n\u001b[0;32m   1038\u001b[0m metric_fn \u001b[38;5;241m=\u001b[39m get_metric_function(\n\u001b[0;32m   1039\u001b[0m     metric, output_shape\u001b[38;5;241m=\u001b[39moutput_shapes[i], loss_fn\u001b[38;5;241m=\u001b[39mloss_fns[i]\n\u001b[0;32m   1040\u001b[0m )\n\u001b[1;32m-> 1041\u001b[0m metric_fn\u001b[38;5;241m.\u001b[39m_from_serialized \u001b[38;5;241m=\u001b[39m from_serialized\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;66;03m# If the metric function is not stateful, we create a stateful\u001b[39;00m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;66;03m# version.\u001b[39;00m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(metric_fn, metrics_module\u001b[38;5;241m.\u001b[39mMetric):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'method' object has no attribute '_from_serialized'"
     ]
    }
   ],
   "source": [
    "# Step 3: Train the autoencoder using audio data\n",
    "logging.info('Training the autoencoder...')\n",
    "autoencoder.train(X_train, batch_size=32, num_epochs=300)\n",
    "logging.info('Training complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aff429cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\AppData\\Local\\Temp\\ipykernel_8936\\1920379383.py:4: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, _ = librosa.load(audio_file, sr=22050)\n",
      "C:\\Users\\gabri\\anaconda3\\lib\\site-packages\\librosa\\core\\audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 72ms/step\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import wavfile\n",
    "# Step 4: Stream audio files through the trained autoencoder and save the encoded audio as WAV\n",
    "def stream_audio_through_autoencoder(audio_file, autoencoder, output_path):\n",
    "    input_audio = preprocess_audio(audio_file)\n",
    "    encoded_audio = autoencoder.predict(np.expand_dims(input_audio, axis=0))\n",
    "    \n",
    "    # Inverse transform the encoded audio back to the waveform\n",
    "    decoded_audio = librosa.feature.inverse.mfcc_to_audio(encoded_audio[0], sr=22050)\n",
    "    \n",
    "    # Scale the audio data to the appropriate range\n",
    "    decoded_audio = (decoded_audio * np.iinfo(np.int16).max).astype(np.int16)\n",
    "    \n",
    "    # Save the decoded audio as a WAV file\n",
    "    wavfile.write(output_path, 22050, decoded_audio)\n",
    "\n",
    "# Specify the output path for the saved WAV file\n",
    "output_path = \"encoded_audio.wav\"\n",
    "\n",
    "# Process an audio file and save the encoded audio as a WAV\n",
    "stream_audio_through_autoencoder(\"Recording (42).m4a\", autoencoder, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7127b05b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
